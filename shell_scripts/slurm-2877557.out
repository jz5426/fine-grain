Some weights of the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ were not used when initializing BertModel: ['encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ and are newly initialized: ['encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Script Parameters:
  FEW_SHOT: 0.01
  FUSION_TYPE: subtraction
  BATCH_SIZE: 1024
  LEARNING_RATE: 0.05
  PATIENCE: 100
  EPOCHS: 800
  PREDICTION_THRESHOLD: 0.5
  TOKEN_MAX_LENGTH: 256
Freezing BERT model
non loaded keys  dict_keys(['patch_local_atten_layer.in_proj_weight', 'patch_local_atten_layer.in_proj_bias', 'patch_local_atten_layer.out_proj.weight', 'patch_local_atten_layer.out_proj.bias', 'word_local_atten_layer.in_proj_weight', 'word_local_atten_layer.in_proj_bias', 'word_local_atten_layer.out_proj.weight', 'word_local_atten_layer.out_proj.bias', 'prototype_layer.weight'])
  INPUT_SIZE: 224
  EXPERIMENT_MODEL: mgca_res50
  MODEL_NAME: mgca
  CACHE_PARENT_DIR: mgca_encoder_features
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]
Encoding train set...
[SUCCESS] Object loaded successfully.
Encoding val set...
[SUCCESS] Object loaded successfully.
Encoding test set...
[SUCCESS] Object loaded successfully.
 Few-shot size: torch.Size([2014]), total training size 201598
Training classifier...
Epoch 0 | Train Loss: 0.7011 | Val Loss: 2.7920
Epoch 1 | Train Loss: 0.6962 | Val Loss: 2.7034
Epoch 2 | Train Loss: 0.6960 | Val Loss: 2.7602
Epoch 3 | Train Loss: 0.6916 | Val Loss: 2.8403
Epoch 4 | Train Loss: 0.6931 | Val Loss: 2.7737
Epoch 5 | Train Loss: 0.6887 | Val Loss: 2.7167
Epoch 6 | Train Loss: 0.6898 | Val Loss: 2.7178
Epoch 7 | Train Loss: 0.6888 | Val Loss: 2.7528
Epoch 8 | Train Loss: 0.6878 | Val Loss: 2.7797
Epoch 9 | Train Loss: 0.6881 | Val Loss: 2.7596
Epoch 10 | Train Loss: 0.6869 | Val Loss: 2.7300
Epoch 11 | Train Loss: 0.6875 | Val Loss: 2.7194
Epoch 12 | Train Loss: 0.6873 | Val Loss: 2.7424
Epoch 13 | Train Loss: 0.6872 | Val Loss: 2.7619
Epoch 14 | Train Loss: 0.6866 | Val Loss: 2.7402
Epoch 15 | Train Loss: 0.6860 | Val Loss: 2.7269
Epoch 16 | Train Loss: 0.6861 | Val Loss: 2.7263
Epoch 17 | Train Loss: 0.6861 | Val Loss: 2.7358
Epoch 18 | Train Loss: 0.6859 | Val Loss: 2.7572
Epoch 19 | Train Loss: 0.6859 | Val Loss: 2.7467
Epoch 20 | Train Loss: 0.6853 | Val Loss: 2.7230
Epoch 21 | Train Loss: 0.6858 | Val Loss: 2.7136
Epoch 22 | Train Loss: 0.6854 | Val Loss: 2.7372
Epoch 23 | Train Loss: 0.6852 | Val Loss: 2.7621
Epoch 24 | Train Loss: 0.6855 | Val Loss: 2.7433
Epoch 25 | Train Loss: 0.6846 | Val Loss: 2.7180
Epoch 26 | Train Loss: 0.6857 | Val Loss: 2.7124
Epoch 27 | Train Loss: 0.6847 | Val Loss: 2.7456
Epoch 28 | Train Loss: 0.6848 | Val Loss: 2.7640
Epoch 29 | Train Loss: 0.6850 | Val Loss: 2.7394
Epoch 30 | Train Loss: 0.6842 | Val Loss: 2.7180
Epoch 31 | Train Loss: 0.6845 | Val Loss: 2.7159
Epoch 32 | Train Loss: 0.6843 | Val Loss: 2.7363
Epoch 33 | Train Loss: 0.6839 | Val Loss: 2.7492
Epoch 34 | Train Loss: 0.6841 | Val Loss: 2.7416
Epoch 35 | Train Loss: 0.6839 | Val Loss: 2.7297
Epoch 36 | Train Loss: 0.6839 | Val Loss: 2.7181
Epoch 37 | Train Loss: 0.6840 | Val Loss: 2.7304
Epoch 38 | Train Loss: 0.6841 | Val Loss: 2.7588
Epoch 39 | Train Loss: 0.6840 | Val Loss: 2.7371
Epoch 40 | Train Loss: 0.6836 | Val Loss: 2.7146
Epoch 41 | Train Loss: 0.6838 | Val Loss: 2.7263
Epoch 42 | Train Loss: 0.6838 | Val Loss: 2.7511
Epoch 43 | Train Loss: 0.6838 | Val Loss: 2.7324
Epoch 44 | Train Loss: 0.6834 | Val Loss: 2.7357
Epoch 45 | Train Loss: 0.6839 | Val Loss: 2.7232
Epoch 46 | Train Loss: 0.6836 | Val Loss: 2.7460
Epoch 47 | Train Loss: 0.6832 | Val Loss: 2.7384
Epoch 48 | Train Loss: 0.6846 | Val Loss: 2.7186
Epoch 49 | Train Loss: 0.6834 | Val Loss: 2.7487
Epoch 50 | Train Loss: 0.6832 | Val Loss: 2.7426
Epoch 51 | Train Loss: 0.6833 | Val Loss: 2.7332
Epoch 52 | Train Loss: 0.6830 | Val Loss: 2.7159
Epoch 53 | Train Loss: 0.6835 | Val Loss: 2.7263
Epoch 54 | Train Loss: 0.6837 | Val Loss: 2.7665
Epoch 55 | Train Loss: 0.6845 | Val Loss: 2.7436
Epoch 56 | Train Loss: 0.6831 | Val Loss: 2.6983
Epoch 57 | Train Loss: 0.6844 | Val Loss: 2.7158
Epoch 58 | Train Loss: 0.6824 | Val Loss: 2.7706
Epoch 59 | Train Loss: 0.6848 | Val Loss: 2.7687
Epoch 60 | Train Loss: 0.6825 | Val Loss: 2.7108
Epoch 61 | Train Loss: 0.6850 | Val Loss: 2.6937
Epoch 62 | Train Loss: 0.6835 | Val Loss: 2.7472
Epoch 63 | Train Loss: 0.6831 | Val Loss: 2.7907
Epoch 64 | Train Loss: 0.6846 | Val Loss: 2.7435
Epoch 65 | Train Loss: 0.6823 | Val Loss: 2.6954
Epoch 66 | Train Loss: 0.6844 | Val Loss: 2.7120
Epoch 67 | Train Loss: 0.6824 | Val Loss: 2.7660
Epoch 68 | Train Loss: 0.6832 | Val Loss: 2.7630
Epoch 69 | Train Loss: 0.6826 | Val Loss: 2.7243
Epoch 70 | Train Loss: 0.6825 | Val Loss: 2.7045
Epoch 71 | Train Loss: 0.6830 | Val Loss: 2.7290
Epoch 72 | Train Loss: 0.6834 | Val Loss: 2.7689
Epoch 73 | Train Loss: 0.6828 | Val Loss: 2.7351
Epoch 74 | Train Loss: 0.6827 | Val Loss: 2.7090
Epoch 75 | Train Loss: 0.6826 | Val Loss: 2.7304
Epoch 76 | Train Loss: 0.6820 | Val Loss: 2.7553
Epoch 77 | Train Loss: 0.6826 | Val Loss: 2.7460
Epoch 78 | Train Loss: 0.6821 | Val Loss: 2.7177
Epoch 79 | Train Loss: 0.6826 | Val Loss: 2.7179
Epoch 80 | Train Loss: 0.6820 | Val Loss: 2.7444
Epoch 81 | Train Loss: 0.6823 | Val Loss: 2.7505
Epoch 82 | Train Loss: 0.6824 | Val Loss: 2.7230
Epoch 83 | Train Loss: 0.6822 | Val Loss: 2.7217
Epoch 84 | Train Loss: 0.6819 | Val Loss: 2.7473
Epoch 85 | Train Loss: 0.6820 | Val Loss: 2.7462
Epoch 86 | Train Loss: 0.6819 | Val Loss: 2.7290
Epoch 87 | Train Loss: 0.6819 | Val Loss: 2.7252
Epoch 88 | Train Loss: 0.6819 | Val Loss: 2.7313
Epoch 89 | Train Loss: 0.6822 | Val Loss: 2.7374
Epoch 90 | Train Loss: 0.6817 | Val Loss: 2.7245
Epoch 91 | Train Loss: 0.6822 | Val Loss: 2.7202
Epoch 92 | Train Loss: 0.6819 | Val Loss: 2.7427
Epoch 93 | Train Loss: 0.6818 | Val Loss: 2.7561
Epoch 94 | Train Loss: 0.6821 | Val Loss: 2.7351
Epoch 95 | Train Loss: 0.6816 | Val Loss: 2.7204
Epoch 96 | Train Loss: 0.6818 | Val Loss: 2.7275
Epoch 97 | Train Loss: 0.6817 | Val Loss: 2.7416
Epoch 98 | Train Loss: 0.6819 | Val Loss: 2.7407
Epoch 99 | Train Loss: 0.6817 | Val Loss: 2.7397
Epoch 100 | Train Loss: 0.6816 | Val Loss: 2.7320
Epoch 101 | Train Loss: 0.6819 | Val Loss: 2.7328
Epoch 102 | Train Loss: 0.6816 | Val Loss: 2.7474
Epoch 103 | Train Loss: 0.6817 | Val Loss: 2.7322
Epoch 104 | Train Loss: 0.6817 | Val Loss: 2.7231
Epoch 105 | Train Loss: 0.6816 | Val Loss: 2.7384
Epoch 106 | Train Loss: 0.6817 | Val Loss: 2.7406
Epoch 107 | Train Loss: 0.6815 | Val Loss: 2.7369
Epoch 108 | Train Loss: 0.6816 | Val Loss: 2.7339
Epoch 109 | Train Loss: 0.6814 | Val Loss: 2.7272
Epoch 110 | Train Loss: 0.6815 | Val Loss: 2.7258
Epoch 111 | Train Loss: 0.6814 | Val Loss: 2.7406
Epoch 112 | Train Loss: 0.6814 | Val Loss: 2.7407
Epoch 113 | Train Loss: 0.6813 | Val Loss: 2.7254
Epoch 114 | Train Loss: 0.6814 | Val Loss: 2.7248
Epoch 115 | Train Loss: 0.6819 | Val Loss: 2.7441
Epoch 116 | Train Loss: 0.6814 | Val Loss: 2.7282
Epoch 117 | Train Loss: 0.6816 | Val Loss: 2.7301
Epoch 118 | Train Loss: 0.6828 | Val Loss: 2.7180
Epoch 119 | Train Loss: 0.6819 | Val Loss: 2.7589
Epoch 120 | Train Loss: 0.6817 | Val Loss: 2.7391
Epoch 121 | Train Loss: 0.6817 | Val Loss: 2.7215
Epoch 122 | Train Loss: 0.6816 | Val Loss: 2.7345
Epoch 123 | Train Loss: 0.6810 | Val Loss: 2.7549
Epoch 124 | Train Loss: 0.6816 | Val Loss: 2.7424
Epoch 125 | Train Loss: 0.6812 | Val Loss: 2.7264
Epoch 126 | Train Loss: 0.6813 | Val Loss: 2.7249
Epoch 127 | Train Loss: 0.6813 | Val Loss: 2.7298
Epoch 128 | Train Loss: 0.6814 | Val Loss: 2.7427
Epoch 129 | Train Loss: 0.6813 | Val Loss: 2.7342
Epoch 130 | Train Loss: 0.6812 | Val Loss: 2.7191
Epoch 131 | Train Loss: 0.6813 | Val Loss: 2.7311
Epoch 132 | Train Loss: 0.6811 | Val Loss: 2.7425
Epoch 133 | Train Loss: 0.6824 | Val Loss: 2.7448
Epoch 134 | Train Loss: 0.6815 | Val Loss: 2.7112
Epoch 135 | Train Loss: 0.6816 | Val Loss: 2.7313
Epoch 136 | Train Loss: 0.6813 | Val Loss: 2.7622
Epoch 137 | Train Loss: 0.6815 | Val Loss: 2.7357
Epoch 138 | Train Loss: 0.6810 | Val Loss: 2.7226
Epoch 139 | Train Loss: 0.6811 | Val Loss: 2.7232
Epoch 140 | Train Loss: 0.6811 | Val Loss: 2.7391
Epoch 141 | Train Loss: 0.6812 | Val Loss: 2.7372
Epoch 142 | Train Loss: 0.6811 | Val Loss: 2.7381
Epoch 143 | Train Loss: 0.6813 | Val Loss: 2.7230
Epoch 144 | Train Loss: 0.6810 | Val Loss: 2.7371
Epoch 145 | Train Loss: 0.6809 | Val Loss: 2.7456
Epoch 146 | Train Loss: 0.6815 | Val Loss: 2.7273
Epoch 147 | Train Loss: 0.6821 | Val Loss: 2.7468
Epoch 148 | Train Loss: 0.6810 | Val Loss: 2.7265
Epoch 149 | Train Loss: 0.6810 | Val Loss: 2.7202
Epoch 150 | Train Loss: 0.6810 | Val Loss: 2.7363
Epoch 151 | Train Loss: 0.6809 | Val Loss: 2.7473
Epoch 152 | Train Loss: 0.6818 | Val Loss: 2.7405
Epoch 153 | Train Loss: 0.6808 | Val Loss: 2.7110
Epoch 154 | Train Loss: 0.6816 | Val Loss: 2.7222
Epoch 155 | Train Loss: 0.6811 | Val Loss: 2.7523
Epoch 156 | Train Loss: 0.6811 | Val Loss: 2.7399
Epoch 157 | Train Loss: 0.6812 | Val Loss: 2.7190
Epoch 158 | Train Loss: 0.6811 | Val Loss: 2.7387
Epoch 159 | Train Loss: 0.6815 | Val Loss: 2.7544
Epoch 160 | Train Loss: 0.6810 | Val Loss: 2.7258
Epoch 161 | Train Loss: 0.6811 | Val Loss: 2.7124
Early stopping.
Test Accuracy: 0.5428
Test PRAUC: 0.5492
Results saved to /cluster/projects/mcintoshgroup/publicData/fine-grain/experiment/mgca_res50_results.csv
Some weights of the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ were not used when initializing BertModel: ['encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'pooler.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ and are newly initialized: ['encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Script Parameters:
  FEW_SHOT: 0.05
  FUSION_TYPE: subtraction
  BATCH_SIZE: 1024
  LEARNING_RATE: 0.05
  PATIENCE: 100
  EPOCHS: 800
  PREDICTION_THRESHOLD: 0.5
  TOKEN_MAX_LENGTH: 256
Freezing BERT model
non loaded keys  dict_keys(['patch_local_atten_layer.in_proj_weight', 'patch_local_atten_layer.in_proj_bias', 'patch_local_atten_layer.out_proj.weight', 'patch_local_atten_layer.out_proj.bias', 'word_local_atten_layer.in_proj_weight', 'word_local_atten_layer.in_proj_bias', 'word_local_atten_layer.out_proj.weight', 'word_local_atten_layer.out_proj.bias', 'prototype_layer.weight'])
  INPUT_SIZE: 224
  EXPERIMENT_MODEL: mgca_res50
  MODEL_NAME: mgca
  CACHE_PARENT_DIR: mgca_encoder_features
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]
Encoding train set...
[SUCCESS] Object loaded successfully.
Encoding val set...
[SUCCESS] Object loaded successfully.
Encoding test set...
[SUCCESS] Object loaded successfully.
 Few-shot size: torch.Size([10078]), total training size 201598
Training classifier...
Epoch 0 | Train Loss: 0.6948 | Val Loss: 2.7625
Epoch 1 | Train Loss: 0.6901 | Val Loss: 2.7441
Epoch 2 | Train Loss: 0.6885 | Val Loss: 2.7129
Epoch 3 | Train Loss: 0.6887 | Val Loss: 2.7037
Epoch 4 | Train Loss: 0.6875 | Val Loss: 2.7035
Epoch 5 | Train Loss: 0.6894 | Val Loss: 2.6916
Epoch 6 | Train Loss: 0.6874 | Val Loss: 2.7219
Epoch 7 | Train Loss: 0.6870 | Val Loss: 2.7027
Epoch 8 | Train Loss: 0.6864 | Val Loss: 2.7556
Epoch 9 | Train Loss: 0.6863 | Val Loss: 2.7658
Epoch 10 | Train Loss: 0.6862 | Val Loss: 2.7086
Epoch 11 | Train Loss: 0.6859 | Val Loss: 2.6822
Epoch 12 | Train Loss: 0.6867 | Val Loss: 2.7116
Epoch 13 | Train Loss: 0.6860 | Val Loss: 2.7180
Epoch 14 | Train Loss: 0.6858 | Val Loss: 2.7319
Epoch 15 | Train Loss: 0.6852 | Val Loss: 2.7455
Epoch 16 | Train Loss: 0.6850 | Val Loss: 2.7247
Epoch 17 | Train Loss: 0.6847 | Val Loss: 2.7555
Epoch 18 | Train Loss: 0.6850 | Val Loss: 2.7629
Epoch 19 | Train Loss: 0.6849 | Val Loss: 2.7434
Epoch 20 | Train Loss: 0.6855 | Val Loss: 2.7500
Epoch 21 | Train Loss: 0.6853 | Val Loss: 2.7211
Epoch 22 | Train Loss: 0.6841 | Val Loss: 2.7732
Epoch 23 | Train Loss: 0.6885 | Val Loss: 2.8852
Epoch 24 | Train Loss: 0.6870 | Val Loss: 2.7904
Epoch 25 | Train Loss: 0.6852 | Val Loss: 2.7514
Epoch 26 | Train Loss: 0.6848 | Val Loss: 2.7228
Epoch 27 | Train Loss: 0.6845 | Val Loss: 2.7317
Epoch 28 | Train Loss: 0.6841 | Val Loss: 2.7348
Epoch 29 | Train Loss: 0.6846 | Val Loss: 2.7611
Epoch 30 | Train Loss: 0.6846 | Val Loss: 2.7592
Epoch 31 | Train Loss: 0.6849 | Val Loss: 2.7539
Epoch 32 | Train Loss: 0.6844 | Val Loss: 2.7372
Epoch 33 | Train Loss: 0.6842 | Val Loss: 2.7066
Epoch 34 | Train Loss: 0.6843 | Val Loss: 2.7546
Epoch 35 | Train Loss: 0.6840 | Val Loss: 2.7777
Epoch 36 | Train Loss: 0.6842 | Val Loss: 2.7243
Epoch 37 | Train Loss: 0.6843 | Val Loss: 2.7339
Epoch 38 | Train Loss: 0.6844 | Val Loss: 2.7362
Epoch 39 | Train Loss: 0.6846 | Val Loss: 2.7583
Epoch 40 | Train Loss: 0.6843 | Val Loss: 2.7682
Epoch 41 | Train Loss: 0.6847 | Val Loss: 2.7475
Epoch 42 | Train Loss: 0.6842 | Val Loss: 2.7807
Epoch 43 | Train Loss: 0.6853 | Val Loss: 2.8143
Epoch 44 | Train Loss: 0.6864 | Val Loss: 2.7539
Epoch 45 | Train Loss: 0.6856 | Val Loss: 2.7720
Epoch 46 | Train Loss: 0.6847 | Val Loss: 2.7634
Epoch 47 | Train Loss: 0.6844 | Val Loss: 2.7578
Epoch 48 | Train Loss: 0.6858 | Val Loss: 2.8427
Epoch 49 | Train Loss: 0.6853 | Val Loss: 2.7346
Epoch 50 | Train Loss: 0.6841 | Val Loss: 2.6877
Epoch 51 | Train Loss: 0.6844 | Val Loss: 2.7109
Epoch 52 | Train Loss: 0.6853 | Val Loss: 2.6864
Epoch 53 | Train Loss: 0.6874 | Val Loss: 2.7252
Epoch 54 | Train Loss: 0.6863 | Val Loss: 2.7509
Epoch 55 | Train Loss: 0.6854 | Val Loss: 2.7772
Epoch 56 | Train Loss: 0.6842 | Val Loss: 2.7461
Epoch 57 | Train Loss: 0.6841 | Val Loss: 2.7452
Epoch 58 | Train Loss: 0.6847 | Val Loss: 2.7580
Epoch 59 | Train Loss: 0.6852 | Val Loss: 2.7775
Epoch 60 | Train Loss: 0.6877 | Val Loss: 2.7848
Epoch 61 | Train Loss: 0.6874 | Val Loss: 2.7409
Epoch 62 | Train Loss: 0.6873 | Val Loss: 2.7123
Epoch 63 | Train Loss: 0.6857 | Val Loss: 2.7404
Epoch 64 | Train Loss: 0.6851 | Val Loss: 2.7247
Epoch 65 | Train Loss: 0.6850 | Val Loss: 2.7207
Epoch 66 | Train Loss: 0.6838 | Val Loss: 2.7251
Epoch 67 | Train Loss: 0.6842 | Val Loss: 2.6934
Epoch 68 | Train Loss: 0.6851 | Val Loss: 2.7202
Epoch 69 | Train Loss: 0.6838 | Val Loss: 2.7066
Epoch 70 | Train Loss: 0.6848 | Val Loss: 2.7120
Epoch 71 | Train Loss: 0.6842 | Val Loss: 2.7530
Epoch 72 | Train Loss: 0.6839 | Val Loss: 2.7486
Epoch 73 | Train Loss: 0.6846 | Val Loss: 2.7859
Epoch 74 | Train Loss: 0.6858 | Val Loss: 2.7447
Epoch 75 | Train Loss: 0.6851 | Val Loss: 2.8225
Epoch 76 | Train Loss: 0.6855 | Val Loss: 2.7465
Epoch 77 | Train Loss: 0.6836 | Val Loss: 2.7229
Epoch 78 | Train Loss: 0.6838 | Val Loss: 2.7366
Epoch 79 | Train Loss: 0.6837 | Val Loss: 2.7104
Epoch 80 | Train Loss: 0.6838 | Val Loss: 2.7842
Epoch 81 | Train Loss: 0.6869 | Val Loss: 2.7688
Epoch 82 | Train Loss: 0.6852 | Val Loss: 2.7664
Epoch 83 | Train Loss: 0.6834 | Val Loss: 2.7739
Epoch 84 | Train Loss: 0.6857 | Val Loss: 2.7723
Epoch 85 | Train Loss: 0.6868 | Val Loss: 2.7099
Epoch 86 | Train Loss: 0.6835 | Val Loss: 2.7320
Epoch 87 | Train Loss: 0.6841 | Val Loss: 2.7236
Epoch 88 | Train Loss: 0.6860 | Val Loss: 2.7723
Epoch 89 | Train Loss: 0.6859 | Val Loss: 2.7479
Epoch 90 | Train Loss: 0.6841 | Val Loss: 2.7591
Epoch 91 | Train Loss: 0.6842 | Val Loss: 2.7537
Epoch 92 | Train Loss: 0.6843 | Val Loss: 2.7604
Epoch 93 | Train Loss: 0.6849 | Val Loss: 2.7647
Epoch 94 | Train Loss: 0.6848 | Val Loss: 2.7254
Epoch 95 | Train Loss: 0.6844 | Val Loss: 2.7265
Epoch 96 | Train Loss: 0.6840 | Val Loss: 2.7023
Epoch 97 | Train Loss: 0.6845 | Val Loss: 2.7030
Epoch 98 | Train Loss: 0.6838 | Val Loss: 2.7087
Epoch 99 | Train Loss: 0.6841 | Val Loss: 2.7510
Epoch 100 | Train Loss: 0.6855 | Val Loss: 2.7422
Epoch 101 | Train Loss: 0.6848 | Val Loss: 2.7737
Epoch 102 | Train Loss: 0.6849 | Val Loss: 2.7212
Epoch 103 | Train Loss: 0.6841 | Val Loss: 2.7301
Epoch 104 | Train Loss: 0.6851 | Val Loss: 2.7231
Epoch 105 | Train Loss: 0.6854 | Val Loss: 2.7208
Epoch 106 | Train Loss: 0.6849 | Val Loss: 2.7175
Epoch 107 | Train Loss: 0.6836 | Val Loss: 2.7464
Epoch 108 | Train Loss: 0.6840 | Val Loss: 2.7483
Epoch 109 | Train Loss: 0.6841 | Val Loss: 2.7134
Epoch 110 | Train Loss: 0.6839 | Val Loss: 2.7355
Epoch 111 | Train Loss: 0.6863 | Val Loss: 2.8283
Early stopping.
Test Accuracy: 0.5408
Test PRAUC: 0.5526
Results saved to /cluster/projects/mcintoshgroup/publicData/fine-grain/experiment/mgca_res50_results.csv
Some weights of the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ were not used when initializing BertModel: ['encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'pooler.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.attention.self.value.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ and are newly initialized: ['encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Script Parameters:
  FEW_SHOT: 0.1
  FUSION_TYPE: subtraction
  BATCH_SIZE: 1024
  LEARNING_RATE: 0.05
  PATIENCE: 100
  EPOCHS: 800
  PREDICTION_THRESHOLD: 0.5
  TOKEN_MAX_LENGTH: 256
Freezing BERT model
non loaded keys  dict_keys(['patch_local_atten_layer.in_proj_weight', 'patch_local_atten_layer.in_proj_bias', 'patch_local_atten_layer.out_proj.weight', 'patch_local_atten_layer.out_proj.bias', 'word_local_atten_layer.in_proj_weight', 'word_local_atten_layer.in_proj_bias', 'word_local_atten_layer.out_proj.weight', 'word_local_atten_layer.out_proj.bias', 'prototype_layer.weight'])
  INPUT_SIZE: 224
  EXPERIMENT_MODEL: mgca_res50
  MODEL_NAME: mgca
  CACHE_PARENT_DIR: mgca_encoder_features
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]
Encoding train set...
[SUCCESS] Object loaded successfully.
Encoding val set...
[SUCCESS] Object loaded successfully.
Encoding test set...
[SUCCESS] Object loaded successfully.
 Few-shot size: torch.Size([20158]), total training size 201598
Training classifier...
Epoch 0 | Train Loss: 0.6949 | Val Loss: 2.8294
Epoch 1 | Train Loss: 0.6904 | Val Loss: 2.7449
Epoch 2 | Train Loss: 0.6888 | Val Loss: 2.7485
Epoch 3 | Train Loss: 0.6885 | Val Loss: 2.7306
Epoch 4 | Train Loss: 0.6867 | Val Loss: 2.7411
Epoch 5 | Train Loss: 0.6870 | Val Loss: 2.7520
Epoch 6 | Train Loss: 0.6872 | Val Loss: 2.7355
Epoch 7 | Train Loss: 0.6867 | Val Loss: 2.7365
Epoch 8 | Train Loss: 0.6872 | Val Loss: 2.7765
Epoch 9 | Train Loss: 0.6864 | Val Loss: 2.8342
Epoch 10 | Train Loss: 0.6866 | Val Loss: 2.7422
Epoch 11 | Train Loss: 0.6861 | Val Loss: 2.7347
Epoch 12 | Train Loss: 0.6858 | Val Loss: 2.7179
Epoch 13 | Train Loss: 0.6858 | Val Loss: 2.7512
Epoch 14 | Train Loss: 0.6858 | Val Loss: 2.7609
Epoch 15 | Train Loss: 0.6857 | Val Loss: 2.7532
Epoch 16 | Train Loss: 0.6856 | Val Loss: 2.7385
Epoch 17 | Train Loss: 0.6856 | Val Loss: 2.7029
Epoch 18 | Train Loss: 0.6854 | Val Loss: 2.7208
Epoch 19 | Train Loss: 0.6851 | Val Loss: 2.7259
Epoch 20 | Train Loss: 0.6856 | Val Loss: 2.7592
Epoch 21 | Train Loss: 0.6854 | Val Loss: 2.7495
Epoch 22 | Train Loss: 0.6848 | Val Loss: 2.7043
Epoch 23 | Train Loss: 0.6864 | Val Loss: 2.7157
Epoch 24 | Train Loss: 0.6853 | Val Loss: 2.7374
Epoch 25 | Train Loss: 0.6850 | Val Loss: 2.7097
Epoch 26 | Train Loss: 0.6846 | Val Loss: 2.7943
Epoch 27 | Train Loss: 0.6863 | Val Loss: 2.7290
Epoch 28 | Train Loss: 0.6851 | Val Loss: 2.7039
Epoch 29 | Train Loss: 0.6850 | Val Loss: 2.7614
Epoch 30 | Train Loss: 0.6854 | Val Loss: 2.7558
Epoch 31 | Train Loss: 0.6863 | Val Loss: 2.7255
Epoch 32 | Train Loss: 0.6855 | Val Loss: 2.7270
Epoch 33 | Train Loss: 0.6855 | Val Loss: 2.7115
Epoch 34 | Train Loss: 0.6854 | Val Loss: 2.7054
Epoch 35 | Train Loss: 0.6850 | Val Loss: 2.7065
Epoch 36 | Train Loss: 0.6849 | Val Loss: 2.7418
Epoch 37 | Train Loss: 0.6848 | Val Loss: 2.7762
Epoch 38 | Train Loss: 0.6867 | Val Loss: 2.7675
Epoch 39 | Train Loss: 0.6868 | Val Loss: 2.7444
Epoch 40 | Train Loss: 0.6856 | Val Loss: 2.7600
Epoch 41 | Train Loss: 0.6860 | Val Loss: 2.7091
Epoch 42 | Train Loss: 0.6843 | Val Loss: 2.7497
Epoch 43 | Train Loss: 0.6847 | Val Loss: 2.7156
Epoch 44 | Train Loss: 0.6853 | Val Loss: 2.7198
Epoch 45 | Train Loss: 0.6854 | Val Loss: 2.7901
Epoch 46 | Train Loss: 0.6864 | Val Loss: 2.7879
Epoch 47 | Train Loss: 0.6866 | Val Loss: 2.7826
Epoch 48 | Train Loss: 0.6866 | Val Loss: 2.7238
Epoch 49 | Train Loss: 0.6875 | Val Loss: 2.6727
Epoch 50 | Train Loss: 0.6897 | Val Loss: 2.6998
Epoch 51 | Train Loss: 0.6868 | Val Loss: 2.7151
Epoch 52 | Train Loss: 0.6849 | Val Loss: 2.7389
Epoch 53 | Train Loss: 0.6861 | Val Loss: 2.7188
Epoch 54 | Train Loss: 0.6862 | Val Loss: 2.6928
Epoch 55 | Train Loss: 0.6858 | Val Loss: 2.6985
Epoch 56 | Train Loss: 0.6856 | Val Loss: 2.7250
Epoch 57 | Train Loss: 0.6867 | Val Loss: 2.6940
Epoch 58 | Train Loss: 0.6848 | Val Loss: 2.7263
Epoch 59 | Train Loss: 0.6846 | Val Loss: 2.7229
Epoch 60 | Train Loss: 0.6854 | Val Loss: 2.7356
Epoch 61 | Train Loss: 0.6849 | Val Loss: 2.6902
Epoch 62 | Train Loss: 0.6845 | Val Loss: 2.7246
Epoch 63 | Train Loss: 0.6855 | Val Loss: 2.7513
Epoch 64 | Train Loss: 0.6856 | Val Loss: 2.7541
Epoch 65 | Train Loss: 0.6853 | Val Loss: 2.7512
Epoch 66 | Train Loss: 0.6851 | Val Loss: 2.7068
Epoch 67 | Train Loss: 0.6858 | Val Loss: 2.7247
Epoch 68 | Train Loss: 0.6857 | Val Loss: 2.7308
Epoch 69 | Train Loss: 0.6858 | Val Loss: 2.6846
Epoch 70 | Train Loss: 0.6871 | Val Loss: 2.6827
Epoch 71 | Train Loss: 0.6850 | Val Loss: 2.7376
Epoch 72 | Train Loss: 0.6850 | Val Loss: 2.6888
Epoch 73 | Train Loss: 0.6864 | Val Loss: 2.7249
Epoch 74 | Train Loss: 0.6845 | Val Loss: 2.7058
Epoch 75 | Train Loss: 0.6860 | Val Loss: 2.7164
Epoch 76 | Train Loss: 0.6849 | Val Loss: 2.7077
Epoch 77 | Train Loss: 0.6852 | Val Loss: 2.7524
Epoch 78 | Train Loss: 0.6852 | Val Loss: 2.7389
Epoch 79 | Train Loss: 0.6847 | Val Loss: 2.7431
Epoch 80 | Train Loss: 0.6850 | Val Loss: 2.7058
Epoch 81 | Train Loss: 0.6853 | Val Loss: 2.7358
Epoch 82 | Train Loss: 0.6852 | Val Loss: 2.7718
Epoch 83 | Train Loss: 0.6855 | Val Loss: 2.7211
Epoch 84 | Train Loss: 0.6852 | Val Loss: 2.7059
Epoch 85 | Train Loss: 0.6852 | Val Loss: 2.6955
Epoch 86 | Train Loss: 0.6865 | Val Loss: 2.7053
Epoch 87 | Train Loss: 0.6848 | Val Loss: 2.6998
Epoch 88 | Train Loss: 0.6846 | Val Loss: 2.7061
Epoch 89 | Train Loss: 0.6850 | Val Loss: 2.7259
Epoch 90 | Train Loss: 0.6864 | Val Loss: 2.6931
Epoch 91 | Train Loss: 0.6852 | Val Loss: 2.7264
Epoch 92 | Train Loss: 0.6844 | Val Loss: 2.7057
Epoch 93 | Train Loss: 0.6846 | Val Loss: 2.7686
Epoch 94 | Train Loss: 0.6867 | Val Loss: 2.7178
Epoch 95 | Train Loss: 0.6855 | Val Loss: 2.7782
Epoch 96 | Train Loss: 0.6858 | Val Loss: 2.7481
Epoch 97 | Train Loss: 0.6867 | Val Loss: 2.7602
Epoch 98 | Train Loss: 0.6862 | Val Loss: 2.7670
Epoch 99 | Train Loss: 0.6853 | Val Loss: 2.8162
Epoch 100 | Train Loss: 0.6891 | Val Loss: 2.7385
Epoch 101 | Train Loss: 0.6879 | Val Loss: 2.7273
Epoch 102 | Train Loss: 0.6871 | Val Loss: 2.7171
Epoch 103 | Train Loss: 0.6863 | Val Loss: 2.7016
Epoch 104 | Train Loss: 0.6865 | Val Loss: 2.7408
Epoch 105 | Train Loss: 0.6846 | Val Loss: 2.7372
Epoch 106 | Train Loss: 0.6856 | Val Loss: 2.8104
Epoch 107 | Train Loss: 0.6869 | Val Loss: 2.7295
Epoch 108 | Train Loss: 0.6858 | Val Loss: 2.7093
Epoch 109 | Train Loss: 0.6851 | Val Loss: 2.6993
Epoch 110 | Train Loss: 0.6851 | Val Loss: 2.7072
Epoch 111 | Train Loss: 0.6849 | Val Loss: 2.7310
Epoch 112 | Train Loss: 0.6852 | Val Loss: 2.7108
Epoch 113 | Train Loss: 0.6849 | Val Loss: 2.7497
Epoch 114 | Train Loss: 0.6861 | Val Loss: 2.8137
Epoch 115 | Train Loss: 0.6869 | Val Loss: 2.7678
Epoch 116 | Train Loss: 0.6851 | Val Loss: 2.7133
Epoch 117 | Train Loss: 0.6856 | Val Loss: 2.7245
Epoch 118 | Train Loss: 0.6862 | Val Loss: 2.7413
Epoch 119 | Train Loss: 0.6855 | Val Loss: 2.7394
Epoch 120 | Train Loss: 0.6857 | Val Loss: 2.6950
Epoch 121 | Train Loss: 0.6857 | Val Loss: 2.7115
Epoch 122 | Train Loss: 0.6853 | Val Loss: 2.7245
Epoch 123 | Train Loss: 0.6850 | Val Loss: 2.7419
Epoch 124 | Train Loss: 0.6850 | Val Loss: 2.7434
Epoch 125 | Train Loss: 0.6847 | Val Loss: 2.7305
Epoch 126 | Train Loss: 0.6851 | Val Loss: 2.7451
Epoch 127 | Train Loss: 0.6860 | Val Loss: 2.7928
Epoch 128 | Train Loss: 0.6852 | Val Loss: 2.7463
Epoch 129 | Train Loss: 0.6853 | Val Loss: 2.7148
Epoch 130 | Train Loss: 0.6847 | Val Loss: 2.6902
Epoch 131 | Train Loss: 0.6857 | Val Loss: 2.7009
Epoch 132 | Train Loss: 0.6859 | Val Loss: 2.7850
Epoch 133 | Train Loss: 0.6853 | Val Loss: 2.7462
Epoch 134 | Train Loss: 0.6852 | Val Loss: 2.7368
Epoch 135 | Train Loss: 0.6861 | Val Loss: 2.7245
Epoch 136 | Train Loss: 0.6876 | Val Loss: 2.7408
Epoch 137 | Train Loss: 0.6856 | Val Loss: 2.6856
Epoch 138 | Train Loss: 0.6866 | Val Loss: 2.6819
Epoch 139 | Train Loss: 0.6854 | Val Loss: 2.7266
Epoch 140 | Train Loss: 0.6852 | Val Loss: 2.7559
Epoch 141 | Train Loss: 0.6850 | Val Loss: 2.7041
Epoch 142 | Train Loss: 0.6849 | Val Loss: 2.7371
Epoch 143 | Train Loss: 0.6848 | Val Loss: 2.7329
Epoch 144 | Train Loss: 0.6852 | Val Loss: 2.7419
Epoch 145 | Train Loss: 0.6854 | Val Loss: 2.8523
Epoch 146 | Train Loss: 0.6863 | Val Loss: 2.7747
Epoch 147 | Train Loss: 0.6867 | Val Loss: 2.7573
Epoch 148 | Train Loss: 0.6852 | Val Loss: 2.7537
Epoch 149 | Train Loss: 0.6852 | Val Loss: 2.7174
Early stopping.
Test Accuracy: 0.5491
Test PRAUC: 0.5542
Results saved to /cluster/projects/mcintoshgroup/publicData/fine-grain/experiment/mgca_res50_results.csv
Some weights of the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ were not used when initializing BertModel: ['encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'pooler.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.output.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ and are newly initialized: ['encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Script Parameters:
  FEW_SHOT: 0.2
  FUSION_TYPE: subtraction
  BATCH_SIZE: 1024
  LEARNING_RATE: 0.05
  PATIENCE: 100
  EPOCHS: 800
  PREDICTION_THRESHOLD: 0.5
  TOKEN_MAX_LENGTH: 256
Freezing BERT model
non loaded keys  dict_keys(['patch_local_atten_layer.in_proj_weight', 'patch_local_atten_layer.in_proj_bias', 'patch_local_atten_layer.out_proj.weight', 'patch_local_atten_layer.out_proj.bias', 'word_local_atten_layer.in_proj_weight', 'word_local_atten_layer.in_proj_bias', 'word_local_atten_layer.out_proj.weight', 'word_local_atten_layer.out_proj.bias', 'prototype_layer.weight'])
  INPUT_SIZE: 224
  EXPERIMENT_MODEL: mgca_res50
  MODEL_NAME: mgca
  CACHE_PARENT_DIR: mgca_encoder_features
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]
Encoding train set...
[SUCCESS] Object loaded successfully.
Encoding val set...
[SUCCESS] Object loaded successfully.
Encoding test set...
[SUCCESS] Object loaded successfully.
 Few-shot size: torch.Size([40318]), total training size 201598
Training classifier...
Epoch 0 | Train Loss: 0.6925 | Val Loss: 2.6995
Epoch 1 | Train Loss: 0.6902 | Val Loss: 2.6986
Epoch 2 | Train Loss: 0.6878 | Val Loss: 2.7193
Epoch 3 | Train Loss: 0.6883 | Val Loss: 2.7649
Epoch 4 | Train Loss: 0.6874 | Val Loss: 2.7628
Epoch 5 | Train Loss: 0.6864 | Val Loss: 2.7389
Epoch 6 | Train Loss: 0.6856 | Val Loss: 2.7621
Epoch 7 | Train Loss: 0.6870 | Val Loss: 2.6933
Epoch 8 | Train Loss: 0.6857 | Val Loss: 2.6953
Epoch 9 | Train Loss: 0.6865 | Val Loss: 2.6948
Epoch 10 | Train Loss: 0.6872 | Val Loss: 2.7170
Epoch 11 | Train Loss: 0.6859 | Val Loss: 2.7448
Epoch 12 | Train Loss: 0.6867 | Val Loss: 2.7330
Epoch 13 | Train Loss: 0.6849 | Val Loss: 2.7210
Epoch 14 | Train Loss: 0.6856 | Val Loss: 2.7259
Epoch 15 | Train Loss: 0.6852 | Val Loss: 2.7692
Epoch 16 | Train Loss: 0.6857 | Val Loss: 2.7627
Epoch 17 | Train Loss: 0.6855 | Val Loss: 2.7306
Epoch 18 | Train Loss: 0.6859 | Val Loss: 2.7712
Epoch 19 | Train Loss: 0.6855 | Val Loss: 2.7045
Epoch 20 | Train Loss: 0.6859 | Val Loss: 2.6955
Epoch 21 | Train Loss: 0.6858 | Val Loss: 2.7373
Epoch 22 | Train Loss: 0.6853 | Val Loss: 2.7571
Epoch 23 | Train Loss: 0.6854 | Val Loss: 2.7038
Epoch 24 | Train Loss: 0.6861 | Val Loss: 2.6808
Epoch 25 | Train Loss: 0.6863 | Val Loss: 2.6824
Epoch 26 | Train Loss: 0.6864 | Val Loss: 2.7755
Epoch 27 | Train Loss: 0.6851 | Val Loss: 2.7475
Epoch 28 | Train Loss: 0.6857 | Val Loss: 2.7135
Epoch 29 | Train Loss: 0.6858 | Val Loss: 2.7915
Epoch 30 | Train Loss: 0.6857 | Val Loss: 2.7225
Epoch 31 | Train Loss: 0.6860 | Val Loss: 2.7771
Epoch 32 | Train Loss: 0.6855 | Val Loss: 2.6902
Epoch 33 | Train Loss: 0.6881 | Val Loss: 2.7762
Epoch 34 | Train Loss: 0.6874 | Val Loss: 2.7203
Epoch 35 | Train Loss: 0.6849 | Val Loss: 2.7098
Epoch 36 | Train Loss: 0.6859 | Val Loss: 2.7398
Epoch 37 | Train Loss: 0.6848 | Val Loss: 2.7505
Epoch 38 | Train Loss: 0.6858 | Val Loss: 2.7600
Epoch 39 | Train Loss: 0.6853 | Val Loss: 2.8097
Epoch 40 | Train Loss: 0.6850 | Val Loss: 2.7438
Epoch 41 | Train Loss: 0.6856 | Val Loss: 2.7018
Epoch 42 | Train Loss: 0.6856 | Val Loss: 2.7207
Epoch 43 | Train Loss: 0.6856 | Val Loss: 2.7622
Epoch 44 | Train Loss: 0.6853 | Val Loss: 2.7558
Epoch 45 | Train Loss: 0.6855 | Val Loss: 2.7224
Epoch 46 | Train Loss: 0.6855 | Val Loss: 2.7009
Epoch 47 | Train Loss: 0.6865 | Val Loss: 2.7481
Epoch 48 | Train Loss: 0.6855 | Val Loss: 2.7375
Epoch 49 | Train Loss: 0.6855 | Val Loss: 2.7098
Epoch 50 | Train Loss: 0.6853 | Val Loss: 2.7137
Epoch 51 | Train Loss: 0.6849 | Val Loss: 2.7699
Epoch 52 | Train Loss: 0.6852 | Val Loss: 2.7479
Epoch 53 | Train Loss: 0.6854 | Val Loss: 2.7180
Epoch 54 | Train Loss: 0.6856 | Val Loss: 2.7382
Epoch 55 | Train Loss: 0.6855 | Val Loss: 2.7794
Epoch 56 | Train Loss: 0.6856 | Val Loss: 2.6959
Epoch 57 | Train Loss: 0.6873 | Val Loss: 2.7513
Epoch 58 | Train Loss: 0.6850 | Val Loss: 2.7193
Epoch 59 | Train Loss: 0.6861 | Val Loss: 2.7511
Epoch 60 | Train Loss: 0.6857 | Val Loss: 2.7667
Epoch 61 | Train Loss: 0.6863 | Val Loss: 2.7333
Epoch 62 | Train Loss: 0.6874 | Val Loss: 2.7051
Epoch 63 | Train Loss: 0.6869 | Val Loss: 2.6807
Epoch 64 | Train Loss: 0.6854 | Val Loss: 2.7324
Epoch 65 | Train Loss: 0.6857 | Val Loss: 2.7478
Epoch 66 | Train Loss: 0.6874 | Val Loss: 2.6874
Epoch 67 | Train Loss: 0.6858 | Val Loss: 2.7749
Epoch 68 | Train Loss: 0.6857 | Val Loss: 2.7133
Epoch 69 | Train Loss: 0.6855 | Val Loss: 2.7095
Epoch 70 | Train Loss: 0.6850 | Val Loss: 2.7008
Epoch 71 | Train Loss: 0.6852 | Val Loss: 2.6959
Epoch 72 | Train Loss: 0.6850 | Val Loss: 2.6897
Epoch 73 | Train Loss: 0.6864 | Val Loss: 2.7335
Epoch 74 | Train Loss: 0.6851 | Val Loss: 2.7021
Epoch 75 | Train Loss: 0.6862 | Val Loss: 2.7180
Epoch 76 | Train Loss: 0.6857 | Val Loss: 2.8130
Epoch 77 | Train Loss: 0.6874 | Val Loss: 2.7826
Epoch 78 | Train Loss: 0.6857 | Val Loss: 2.7846
Epoch 79 | Train Loss: 0.6855 | Val Loss: 2.7265
Epoch 80 | Train Loss: 0.6868 | Val Loss: 2.6876
Epoch 81 | Train Loss: 0.6860 | Val Loss: 2.7625
Epoch 82 | Train Loss: 0.6857 | Val Loss: 2.8009
Epoch 83 | Train Loss: 0.6859 | Val Loss: 2.7014
Epoch 84 | Train Loss: 0.6848 | Val Loss: 2.7880
Epoch 85 | Train Loss: 0.6871 | Val Loss: 2.8241
Epoch 86 | Train Loss: 0.6856 | Val Loss: 2.7072
Epoch 87 | Train Loss: 0.6856 | Val Loss: 2.7643
Epoch 88 | Train Loss: 0.6860 | Val Loss: 2.7692
Epoch 89 | Train Loss: 0.6856 | Val Loss: 2.7888
Epoch 90 | Train Loss: 0.6849 | Val Loss: 2.7276
Epoch 91 | Train Loss: 0.6847 | Val Loss: 2.7137
Epoch 92 | Train Loss: 0.6870 | Val Loss: 2.8080
Epoch 93 | Train Loss: 0.6864 | Val Loss: 2.6969
Epoch 94 | Train Loss: 0.6853 | Val Loss: 2.7567
Epoch 95 | Train Loss: 0.6851 | Val Loss: 2.7084
Epoch 96 | Train Loss: 0.6860 | Val Loss: 2.6923
Epoch 97 | Train Loss: 0.6857 | Val Loss: 2.7584
Epoch 98 | Train Loss: 0.6857 | Val Loss: 2.8072
Epoch 99 | Train Loss: 0.6858 | Val Loss: 2.6965
Epoch 100 | Train Loss: 0.6851 | Val Loss: 2.7051
Epoch 101 | Train Loss: 0.6851 | Val Loss: 2.7625
Epoch 102 | Train Loss: 0.6862 | Val Loss: 2.7552
Epoch 103 | Train Loss: 0.6864 | Val Loss: 2.7090
Epoch 104 | Train Loss: 0.6850 | Val Loss: 2.7279
Epoch 105 | Train Loss: 0.6853 | Val Loss: 2.7835
Epoch 106 | Train Loss: 0.6856 | Val Loss: 2.7748
Epoch 107 | Train Loss: 0.6857 | Val Loss: 2.7459
Epoch 108 | Train Loss: 0.6851 | Val Loss: 2.7379
Epoch 109 | Train Loss: 0.6861 | Val Loss: 2.7968
Epoch 110 | Train Loss: 0.6863 | Val Loss: 2.7765
Epoch 111 | Train Loss: 0.6856 | Val Loss: 2.7014
Epoch 112 | Train Loss: 0.6854 | Val Loss: 2.7320
Epoch 113 | Train Loss: 0.6861 | Val Loss: 2.7804
Epoch 114 | Train Loss: 0.6861 | Val Loss: 2.7409
Epoch 115 | Train Loss: 0.6856 | Val Loss: 2.7209
Epoch 116 | Train Loss: 0.6856 | Val Loss: 2.7506
Epoch 117 | Train Loss: 0.6858 | Val Loss: 2.7672
Epoch 118 | Train Loss: 0.6857 | Val Loss: 2.7666
Epoch 119 | Train Loss: 0.6863 | Val Loss: 2.7175
Epoch 120 | Train Loss: 0.6884 | Val Loss: 2.7155
Epoch 121 | Train Loss: 0.6867 | Val Loss: 2.7878
Epoch 122 | Train Loss: 0.6855 | Val Loss: 2.7412
Epoch 123 | Train Loss: 0.6865 | Val Loss: 2.7660
Epoch 124 | Train Loss: 0.6859 | Val Loss: 2.7741
Epoch 125 | Train Loss: 0.6848 | Val Loss: 2.7285
Epoch 126 | Train Loss: 0.6850 | Val Loss: 2.7637
Epoch 127 | Train Loss: 0.6885 | Val Loss: 2.8502
Epoch 128 | Train Loss: 0.6864 | Val Loss: 2.7346
Epoch 129 | Train Loss: 0.6853 | Val Loss: 2.7061
Epoch 130 | Train Loss: 0.6867 | Val Loss: 2.7046
Epoch 131 | Train Loss: 0.6855 | Val Loss: 2.7652
Epoch 132 | Train Loss: 0.6874 | Val Loss: 2.7207
Epoch 133 | Train Loss: 0.6877 | Val Loss: 2.7006
Epoch 134 | Train Loss: 0.6857 | Val Loss: 2.7367
Epoch 135 | Train Loss: 0.6852 | Val Loss: 2.7289
Epoch 136 | Train Loss: 0.6850 | Val Loss: 2.7230
Epoch 137 | Train Loss: 0.6848 | Val Loss: 2.7311
Epoch 138 | Train Loss: 0.6862 | Val Loss: 2.7553
Epoch 139 | Train Loss: 0.6853 | Val Loss: 2.7945
Epoch 140 | Train Loss: 0.6861 | Val Loss: 2.7793
Epoch 141 | Train Loss: 0.6856 | Val Loss: 2.7559
Epoch 142 | Train Loss: 0.6853 | Val Loss: 2.8056
Epoch 143 | Train Loss: 0.6851 | Val Loss: 2.7512
Epoch 144 | Train Loss: 0.6854 | Val Loss: 2.7668
Epoch 145 | Train Loss: 0.6852 | Val Loss: 2.7089
Epoch 146 | Train Loss: 0.6849 | Val Loss: 2.7397
Epoch 147 | Train Loss: 0.6857 | Val Loss: 2.7133
Epoch 148 | Train Loss: 0.6852 | Val Loss: 2.7760
Epoch 149 | Train Loss: 0.6859 | Val Loss: 2.7406
Epoch 150 | Train Loss: 0.6855 | Val Loss: 2.7310
Epoch 151 | Train Loss: 0.6850 | Val Loss: 2.7331
Epoch 152 | Train Loss: 0.6857 | Val Loss: 2.7765
Epoch 153 | Train Loss: 0.6859 | Val Loss: 2.6796
Epoch 154 | Train Loss: 0.6859 | Val Loss: 2.7421
Epoch 155 | Train Loss: 0.6856 | Val Loss: 2.7114
Epoch 156 | Train Loss: 0.6852 | Val Loss: 2.8178
Epoch 157 | Train Loss: 0.6857 | Val Loss: 2.7751
Epoch 158 | Train Loss: 0.6857 | Val Loss: 2.6917
Epoch 159 | Train Loss: 0.6857 | Val Loss: 2.7805
Epoch 160 | Train Loss: 0.6862 | Val Loss: 2.6915
Epoch 161 | Train Loss: 0.6850 | Val Loss: 2.6980
Epoch 162 | Train Loss: 0.6860 | Val Loss: 2.7149
Epoch 163 | Train Loss: 0.6850 | Val Loss: 2.7121
Epoch 164 | Train Loss: 0.6852 | Val Loss: 2.8072
Epoch 165 | Train Loss: 0.6855 | Val Loss: 2.7557
Epoch 166 | Train Loss: 0.6856 | Val Loss: 2.7572
Epoch 167 | Train Loss: 0.6855 | Val Loss: 2.6897
Epoch 168 | Train Loss: 0.6852 | Val Loss: 2.7537
Epoch 169 | Train Loss: 0.6856 | Val Loss: 2.8371
Epoch 170 | Train Loss: 0.6861 | Val Loss: 2.7111
Epoch 171 | Train Loss: 0.6869 | Val Loss: 2.7278
Epoch 172 | Train Loss: 0.6887 | Val Loss: 2.6837
Epoch 173 | Train Loss: 0.6863 | Val Loss: 2.7124
Epoch 174 | Train Loss: 0.6852 | Val Loss: 2.7588
Epoch 175 | Train Loss: 0.6847 | Val Loss: 2.7690
Epoch 176 | Train Loss: 0.6872 | Val Loss: 2.7456
Epoch 177 | Train Loss: 0.6851 | Val Loss: 2.7224
Epoch 178 | Train Loss: 0.6856 | Val Loss: 2.8238
Epoch 179 | Train Loss: 0.6861 | Val Loss: 2.7363
Epoch 180 | Train Loss: 0.6857 | Val Loss: 2.7408
Epoch 181 | Train Loss: 0.6852 | Val Loss: 2.7252
Epoch 182 | Train Loss: 0.6859 | Val Loss: 2.7513
Epoch 183 | Train Loss: 0.6856 | Val Loss: 2.6763
Epoch 184 | Train Loss: 0.6864 | Val Loss: 2.7283
Epoch 185 | Train Loss: 0.6859 | Val Loss: 2.7247
Epoch 186 | Train Loss: 0.6857 | Val Loss: 2.7557
Epoch 187 | Train Loss: 0.6857 | Val Loss: 2.7687
Epoch 188 | Train Loss: 0.6850 | Val Loss: 2.7774
Epoch 189 | Train Loss: 0.6849 | Val Loss: 2.7438
Epoch 190 | Train Loss: 0.6858 | Val Loss: 2.7646
Epoch 191 | Train Loss: 0.6854 | Val Loss: 2.7305
Epoch 192 | Train Loss: 0.6855 | Val Loss: 2.7404
Epoch 193 | Train Loss: 0.6856 | Val Loss: 2.7425
Epoch 194 | Train Loss: 0.6854 | Val Loss: 2.7654
Epoch 195 | Train Loss: 0.6854 | Val Loss: 2.7494
Epoch 196 | Train Loss: 0.6856 | Val Loss: 2.7214
Epoch 197 | Train Loss: 0.6852 | Val Loss: 2.7299
Epoch 198 | Train Loss: 0.6854 | Val Loss: 2.7280
Epoch 199 | Train Loss: 0.6853 | Val Loss: 2.6857
Epoch 200 | Train Loss: 0.6863 | Val Loss: 2.7338
Epoch 201 | Train Loss: 0.6856 | Val Loss: 2.8198
Epoch 202 | Train Loss: 0.6870 | Val Loss: 2.7931
Epoch 203 | Train Loss: 0.6865 | Val Loss: 2.7363
Epoch 204 | Train Loss: 0.6847 | Val Loss: 2.7326
Epoch 205 | Train Loss: 0.6860 | Val Loss: 2.7252
Epoch 206 | Train Loss: 0.6850 | Val Loss: 2.7530
Epoch 207 | Train Loss: 0.6851 | Val Loss: 2.7253
Epoch 208 | Train Loss: 0.6856 | Val Loss: 2.7434
Epoch 209 | Train Loss: 0.6852 | Val Loss: 2.7193
Epoch 210 | Train Loss: 0.6855 | Val Loss: 2.7452
Epoch 211 | Train Loss: 0.6859 | Val Loss: 2.7469
Epoch 212 | Train Loss: 0.6856 | Val Loss: 2.7400
Epoch 213 | Train Loss: 0.6855 | Val Loss: 2.7038
Epoch 214 | Train Loss: 0.6851 | Val Loss: 2.7628
Epoch 215 | Train Loss: 0.6856 | Val Loss: 2.7415
Epoch 216 | Train Loss: 0.6851 | Val Loss: 2.7313
Epoch 217 | Train Loss: 0.6848 | Val Loss: 2.7498
Epoch 218 | Train Loss: 0.6854 | Val Loss: 2.7381
Epoch 219 | Train Loss: 0.6858 | Val Loss: 2.6964
Epoch 220 | Train Loss: 0.6861 | Val Loss: 2.8342
Epoch 221 | Train Loss: 0.6862 | Val Loss: 2.7493
Epoch 222 | Train Loss: 0.6854 | Val Loss: 2.6943
Epoch 223 | Train Loss: 0.6879 | Val Loss: 2.6980
Epoch 224 | Train Loss: 0.6866 | Val Loss: 2.6980
Epoch 225 | Train Loss: 0.6869 | Val Loss: 2.8229
Epoch 226 | Train Loss: 0.6877 | Val Loss: 2.7367
Epoch 227 | Train Loss: 0.6889 | Val Loss: 2.6962
Epoch 228 | Train Loss: 0.6863 | Val Loss: 2.7655
Epoch 229 | Train Loss: 0.6850 | Val Loss: 2.7198
Epoch 230 | Train Loss: 0.6851 | Val Loss: 2.6932
Epoch 231 | Train Loss: 0.6855 | Val Loss: 2.6950
Epoch 232 | Train Loss: 0.6860 | Val Loss: 2.7402
Epoch 233 | Train Loss: 0.6852 | Val Loss: 2.7223
Epoch 234 | Train Loss: 0.6863 | Val Loss: 2.7460
Epoch 235 | Train Loss: 0.6862 | Val Loss: 2.7490
Epoch 236 | Train Loss: 0.6860 | Val Loss: 2.7485
Epoch 237 | Train Loss: 0.6862 | Val Loss: 2.7536
Epoch 238 | Train Loss: 0.6861 | Val Loss: 2.7840
Epoch 239 | Train Loss: 0.6854 | Val Loss: 2.8099
Epoch 240 | Train Loss: 0.6853 | Val Loss: 2.7423
Epoch 241 | Train Loss: 0.6852 | Val Loss: 2.7694
Epoch 242 | Train Loss: 0.6861 | Val Loss: 2.7295
Epoch 243 | Train Loss: 0.6855 | Val Loss: 2.7365
Epoch 244 | Train Loss: 0.6869 | Val Loss: 2.6800
Epoch 245 | Train Loss: 0.6864 | Val Loss: 2.7431
Epoch 246 | Train Loss: 0.6855 | Val Loss: 2.6941
Epoch 247 | Train Loss: 0.6847 | Val Loss: 2.6957
Epoch 248 | Train Loss: 0.6857 | Val Loss: 2.8146
Epoch 249 | Train Loss: 0.6851 | Val Loss: 2.7358
Epoch 250 | Train Loss: 0.6852 | Val Loss: 2.7296
Epoch 251 | Train Loss: 0.6855 | Val Loss: 2.7067
Epoch 252 | Train Loss: 0.6857 | Val Loss: 2.7449
Epoch 253 | Train Loss: 0.6851 | Val Loss: 2.7002
Epoch 254 | Train Loss: 0.6849 | Val Loss: 2.7272
Epoch 255 | Train Loss: 0.6850 | Val Loss: 2.7182
Epoch 256 | Train Loss: 0.6870 | Val Loss: 2.6890
Epoch 257 | Train Loss: 0.6859 | Val Loss: 2.6838
Epoch 258 | Train Loss: 0.6863 | Val Loss: 2.7199
Epoch 259 | Train Loss: 0.6851 | Val Loss: 2.7498
Epoch 260 | Train Loss: 0.6855 | Val Loss: 2.7801
Epoch 261 | Train Loss: 0.6864 | Val Loss: 2.7545
Epoch 262 | Train Loss: 0.6883 | Val Loss: 2.7244
Epoch 263 | Train Loss: 0.6850 | Val Loss: 2.7260
Epoch 264 | Train Loss: 0.6863 | Val Loss: 2.7098
Epoch 265 | Train Loss: 0.6858 | Val Loss: 2.7212
Epoch 266 | Train Loss: 0.6850 | Val Loss: 2.7377
Epoch 267 | Train Loss: 0.6848 | Val Loss: 2.8113
Epoch 268 | Train Loss: 0.6858 | Val Loss: 2.6978
Epoch 269 | Train Loss: 0.6851 | Val Loss: 2.8144
Epoch 270 | Train Loss: 0.6866 | Val Loss: 2.7034
Epoch 271 | Train Loss: 0.6853 | Val Loss: 2.7094
Epoch 272 | Train Loss: 0.6872 | Val Loss: 2.6904
Epoch 273 | Train Loss: 0.6856 | Val Loss: 2.7516
Epoch 274 | Train Loss: 0.6852 | Val Loss: 2.8369
Epoch 275 | Train Loss: 0.6863 | Val Loss: 2.7058
Epoch 276 | Train Loss: 0.6861 | Val Loss: 2.7602
Epoch 277 | Train Loss: 0.6855 | Val Loss: 2.7316
Epoch 278 | Train Loss: 0.6849 | Val Loss: 2.7304
Epoch 279 | Train Loss: 0.6859 | Val Loss: 2.6956
Epoch 280 | Train Loss: 0.6851 | Val Loss: 2.7225
Epoch 281 | Train Loss: 0.6856 | Val Loss: 2.8079
Epoch 282 | Train Loss: 0.6860 | Val Loss: 2.7385
Epoch 283 | Train Loss: 0.6854 | Val Loss: 2.7593
Early stopping.
Test Accuracy: 0.5528
Test PRAUC: 0.5538
Results saved to /cluster/projects/mcintoshgroup/publicData/fine-grain/experiment/mgca_res50_results.csv
Some weights of the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ were not used when initializing BertModel: ['encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'pooler.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ and are newly initialized: ['encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Script Parameters:
  FEW_SHOT: 0.5
  FUSION_TYPE: subtraction
  BATCH_SIZE: 1024
  LEARNING_RATE: 0.05
  PATIENCE: 100
  EPOCHS: 800
  PREDICTION_THRESHOLD: 0.5
  TOKEN_MAX_LENGTH: 256
Freezing BERT model
non loaded keys  dict_keys(['patch_local_atten_layer.in_proj_weight', 'patch_local_atten_layer.in_proj_bias', 'patch_local_atten_layer.out_proj.weight', 'patch_local_atten_layer.out_proj.bias', 'word_local_atten_layer.in_proj_weight', 'word_local_atten_layer.in_proj_bias', 'word_local_atten_layer.out_proj.weight', 'word_local_atten_layer.out_proj.bias', 'prototype_layer.weight'])
  INPUT_SIZE: 224
  EXPERIMENT_MODEL: mgca_res50
  MODEL_NAME: mgca
  CACHE_PARENT_DIR: mgca_encoder_features
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]
Encoding train set...
[SUCCESS] Object loaded successfully.
Encoding val set...
[SUCCESS] Object loaded successfully.
Encoding test set...
[SUCCESS] Object loaded successfully.
 Few-shot size: torch.Size([100798]), total training size 201598
Training classifier...
Epoch 0 | Train Loss: 0.6904 | Val Loss: 2.8016
Epoch 1 | Train Loss: 0.6875 | Val Loss: 2.8026
Epoch 2 | Train Loss: 0.6870 | Val Loss: 2.7328
Epoch 3 | Train Loss: 0.6863 | Val Loss: 2.6974
Epoch 4 | Train Loss: 0.6862 | Val Loss: 2.7406
Epoch 5 | Train Loss: 0.6863 | Val Loss: 2.7632
Epoch 6 | Train Loss: 0.6861 | Val Loss: 2.7316
Epoch 7 | Train Loss: 0.6873 | Val Loss: 2.6779
Epoch 8 | Train Loss: 0.6860 | Val Loss: 2.7153
Epoch 9 | Train Loss: 0.6859 | Val Loss: 2.6938
Epoch 10 | Train Loss: 0.6858 | Val Loss: 2.7278
Epoch 11 | Train Loss: 0.6865 | Val Loss: 2.7004
Epoch 12 | Train Loss: 0.6866 | Val Loss: 2.7284
Epoch 13 | Train Loss: 0.6867 | Val Loss: 2.7208
Epoch 14 | Train Loss: 0.6858 | Val Loss: 2.7248
Epoch 15 | Train Loss: 0.6861 | Val Loss: 2.7725
Epoch 16 | Train Loss: 0.6858 | Val Loss: 2.7173
Epoch 17 | Train Loss: 0.6859 | Val Loss: 2.7732
Epoch 18 | Train Loss: 0.6865 | Val Loss: 2.7475
Epoch 19 | Train Loss: 0.6857 | Val Loss: 2.7204
Epoch 20 | Train Loss: 0.6855 | Val Loss: 2.7052
Epoch 21 | Train Loss: 0.6865 | Val Loss: 2.7998
Epoch 22 | Train Loss: 0.6855 | Val Loss: 2.7202
Epoch 23 | Train Loss: 0.6861 | Val Loss: 2.6925
Epoch 24 | Train Loss: 0.6861 | Val Loss: 2.7160
Epoch 25 | Train Loss: 0.6866 | Val Loss: 2.7431
Epoch 26 | Train Loss: 0.6858 | Val Loss: 2.7354
Epoch 27 | Train Loss: 0.6858 | Val Loss: 2.7397
Epoch 28 | Train Loss: 0.6861 | Val Loss: 2.6999
Epoch 29 | Train Loss: 0.6855 | Val Loss: 2.7110
Epoch 30 | Train Loss: 0.6863 | Val Loss: 2.7405
Epoch 31 | Train Loss: 0.6866 | Val Loss: 2.6876
Epoch 32 | Train Loss: 0.6876 | Val Loss: 2.6778
Epoch 33 | Train Loss: 0.6866 | Val Loss: 2.7707
Epoch 34 | Train Loss: 0.6862 | Val Loss: 2.7647
Epoch 35 | Train Loss: 0.6861 | Val Loss: 2.6861
Epoch 36 | Train Loss: 0.6861 | Val Loss: 2.7605
Epoch 37 | Train Loss: 0.6870 | Val Loss: 2.6972
Epoch 38 | Train Loss: 0.6858 | Val Loss: 2.7503
Epoch 39 | Train Loss: 0.6863 | Val Loss: 2.7063
Epoch 40 | Train Loss: 0.6865 | Val Loss: 2.7371
Epoch 41 | Train Loss: 0.6855 | Val Loss: 2.7223
Epoch 42 | Train Loss: 0.6863 | Val Loss: 2.7393
Epoch 43 | Train Loss: 0.6864 | Val Loss: 2.7425
Epoch 44 | Train Loss: 0.6861 | Val Loss: 2.7412
Epoch 45 | Train Loss: 0.6856 | Val Loss: 2.7901
Epoch 46 | Train Loss: 0.6868 | Val Loss: 2.8074
Epoch 47 | Train Loss: 0.6863 | Val Loss: 2.7008
Epoch 48 | Train Loss: 0.6854 | Val Loss: 2.7132
Epoch 49 | Train Loss: 0.6860 | Val Loss: 2.7849
Epoch 50 | Train Loss: 0.6859 | Val Loss: 2.7367
Epoch 51 | Train Loss: 0.6883 | Val Loss: 2.7629
Epoch 52 | Train Loss: 0.6865 | Val Loss: 2.6969
Epoch 53 | Train Loss: 0.6857 | Val Loss: 2.7327
Epoch 54 | Train Loss: 0.6863 | Val Loss: 2.8203
Epoch 55 | Train Loss: 0.6866 | Val Loss: 2.7533
Epoch 56 | Train Loss: 0.6863 | Val Loss: 2.7084
Epoch 57 | Train Loss: 0.6855 | Val Loss: 2.7338
Epoch 58 | Train Loss: 0.6857 | Val Loss: 2.7146
Epoch 59 | Train Loss: 0.6862 | Val Loss: 2.7051
Epoch 60 | Train Loss: 0.6861 | Val Loss: 2.7472
Epoch 61 | Train Loss: 0.6859 | Val Loss: 2.7362
Epoch 62 | Train Loss: 0.6860 | Val Loss: 2.7291
Epoch 63 | Train Loss: 0.6861 | Val Loss: 2.6870
Epoch 64 | Train Loss: 0.6857 | Val Loss: 2.7110
Epoch 65 | Train Loss: 0.6855 | Val Loss: 2.8080
Epoch 66 | Train Loss: 0.6858 | Val Loss: 2.7342
Epoch 67 | Train Loss: 0.6863 | Val Loss: 2.7382
Epoch 68 | Train Loss: 0.6862 | Val Loss: 2.6923
Epoch 69 | Train Loss: 0.6867 | Val Loss: 2.7299
Epoch 70 | Train Loss: 0.6860 | Val Loss: 2.7586
Epoch 71 | Train Loss: 0.6863 | Val Loss: 2.7677
Epoch 72 | Train Loss: 0.6863 | Val Loss: 2.7146
Epoch 73 | Train Loss: 0.6855 | Val Loss: 2.7332
Epoch 74 | Train Loss: 0.6859 | Val Loss: 2.7142
Epoch 75 | Train Loss: 0.6856 | Val Loss: 2.7299
Epoch 76 | Train Loss: 0.6862 | Val Loss: 2.7702
Epoch 77 | Train Loss: 0.6864 | Val Loss: 2.8330
Epoch 78 | Train Loss: 0.6869 | Val Loss: 2.8025
Epoch 79 | Train Loss: 0.6873 | Val Loss: 2.6798
Epoch 80 | Train Loss: 0.6861 | Val Loss: 2.6881
Epoch 81 | Train Loss: 0.6866 | Val Loss: 2.8008
Epoch 82 | Train Loss: 0.6861 | Val Loss: 2.7876
Epoch 83 | Train Loss: 0.6859 | Val Loss: 2.7521
Epoch 84 | Train Loss: 0.6857 | Val Loss: 2.7320
Epoch 85 | Train Loss: 0.6859 | Val Loss: 2.7351
Epoch 86 | Train Loss: 0.6869 | Val Loss: 2.7591
Epoch 87 | Train Loss: 0.6858 | Val Loss: 2.7397
Epoch 88 | Train Loss: 0.6857 | Val Loss: 2.7281
Epoch 89 | Train Loss: 0.6862 | Val Loss: 2.7350
Epoch 90 | Train Loss: 0.6861 | Val Loss: 2.6863
Epoch 91 | Train Loss: 0.6863 | Val Loss: 2.7550
Epoch 92 | Train Loss: 0.6871 | Val Loss: 2.7603
Epoch 93 | Train Loss: 0.6869 | Val Loss: 2.6863
Epoch 94 | Train Loss: 0.6865 | Val Loss: 2.7363
Epoch 95 | Train Loss: 0.6859 | Val Loss: 2.7168
Epoch 96 | Train Loss: 0.6874 | Val Loss: 2.8315
Epoch 97 | Train Loss: 0.6873 | Val Loss: 2.7591
Epoch 98 | Train Loss: 0.6861 | Val Loss: 2.7118
Epoch 99 | Train Loss: 0.6863 | Val Loss: 2.7299
Epoch 100 | Train Loss: 0.6857 | Val Loss: 2.7687
Epoch 101 | Train Loss: 0.6862 | Val Loss: 2.7483
Epoch 102 | Train Loss: 0.6864 | Val Loss: 2.7998
Epoch 103 | Train Loss: 0.6862 | Val Loss: 2.7490
Epoch 104 | Train Loss: 0.6860 | Val Loss: 2.7356
Epoch 105 | Train Loss: 0.6859 | Val Loss: 2.7340
Epoch 106 | Train Loss: 0.6853 | Val Loss: 2.7587
Epoch 107 | Train Loss: 0.6860 | Val Loss: 2.7783
Epoch 108 | Train Loss: 0.6863 | Val Loss: 2.6955
Epoch 109 | Train Loss: 0.6860 | Val Loss: 2.7544
Epoch 110 | Train Loss: 0.6861 | Val Loss: 2.7430
Epoch 111 | Train Loss: 0.6860 | Val Loss: 2.6820
Epoch 112 | Train Loss: 0.6858 | Val Loss: 2.7364
Epoch 113 | Train Loss: 0.6862 | Val Loss: 2.7163
Epoch 114 | Train Loss: 0.6863 | Val Loss: 2.7845
Epoch 115 | Train Loss: 0.6857 | Val Loss: 2.7662
Epoch 116 | Train Loss: 0.6866 | Val Loss: 2.7041
Epoch 117 | Train Loss: 0.6862 | Val Loss: 2.7542
Epoch 118 | Train Loss: 0.6858 | Val Loss: 2.7072
Epoch 119 | Train Loss: 0.6857 | Val Loss: 2.7544
Epoch 120 | Train Loss: 0.6862 | Val Loss: 2.7402
Epoch 121 | Train Loss: 0.6862 | Val Loss: 2.6809
Epoch 122 | Train Loss: 0.6870 | Val Loss: 2.7328
Epoch 123 | Train Loss: 0.6859 | Val Loss: 2.7940
Epoch 124 | Train Loss: 0.6857 | Val Loss: 2.7719
Epoch 125 | Train Loss: 0.6862 | Val Loss: 2.7474
Epoch 126 | Train Loss: 0.6856 | Val Loss: 2.7037
Epoch 127 | Train Loss: 0.6866 | Val Loss: 2.7783
Epoch 128 | Train Loss: 0.6862 | Val Loss: 2.8136
Epoch 129 | Train Loss: 0.6867 | Val Loss: 2.8068
Epoch 130 | Train Loss: 0.6865 | Val Loss: 2.7069
Epoch 131 | Train Loss: 0.6863 | Val Loss: 2.7547
Epoch 132 | Train Loss: 0.6860 | Val Loss: 2.7702
Early stopping.
Test Accuracy: 0.5465
Test PRAUC: 0.5511
Results saved to /cluster/projects/mcintoshgroup/publicData/fine-grain/experiment/mgca_res50_results.csv
Some weights of the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ were not used when initializing BertModel: ['pooler.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ and are newly initialized: ['encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Script Parameters:
  FEW_SHOT: 0.8
  FUSION_TYPE: subtraction
  BATCH_SIZE: 1024
  LEARNING_RATE: 0.05
  PATIENCE: 100
  EPOCHS: 800
  PREDICTION_THRESHOLD: 0.5
  TOKEN_MAX_LENGTH: 256
Freezing BERT model
non loaded keys  dict_keys(['patch_local_atten_layer.in_proj_weight', 'patch_local_atten_layer.in_proj_bias', 'patch_local_atten_layer.out_proj.weight', 'patch_local_atten_layer.out_proj.bias', 'word_local_atten_layer.in_proj_weight', 'word_local_atten_layer.in_proj_bias', 'word_local_atten_layer.out_proj.weight', 'word_local_atten_layer.out_proj.bias', 'prototype_layer.weight'])
  INPUT_SIZE: 224
  EXPERIMENT_MODEL: mgca_res50
  MODEL_NAME: mgca
  CACHE_PARENT_DIR: mgca_encoder_features
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]
Encoding train set...
[SUCCESS] Object loaded successfully.
Encoding val set...
[SUCCESS] Object loaded successfully.
Encoding test set...
[SUCCESS] Object loaded successfully.
 Few-shot size: torch.Size([161278]), total training size 201598
Training classifier...
Epoch 0 | Train Loss: 0.6885 | Val Loss: 2.7192
Epoch 1 | Train Loss: 0.6860 | Val Loss: 2.7224
Epoch 2 | Train Loss: 0.6861 | Val Loss: 2.7024
Epoch 3 | Train Loss: 0.6856 | Val Loss: 2.6949
Epoch 4 | Train Loss: 0.6861 | Val Loss: 2.7156
Epoch 5 | Train Loss: 0.6861 | Val Loss: 2.7193
Epoch 6 | Train Loss: 0.6856 | Val Loss: 2.7481
Epoch 7 | Train Loss: 0.6855 | Val Loss: 2.7459
Epoch 8 | Train Loss: 0.6858 | Val Loss: 2.7786
Epoch 9 | Train Loss: 0.6861 | Val Loss: 2.6873
Epoch 10 | Train Loss: 0.6856 | Val Loss: 2.7467
Epoch 11 | Train Loss: 0.6865 | Val Loss: 2.7515
Epoch 12 | Train Loss: 0.6855 | Val Loss: 2.7321
Epoch 13 | Train Loss: 0.6859 | Val Loss: 2.6808
Epoch 14 | Train Loss: 0.6857 | Val Loss: 2.7516
Epoch 15 | Train Loss: 0.6857 | Val Loss: 2.7315
Epoch 16 | Train Loss: 0.6857 | Val Loss: 2.7023
Epoch 17 | Train Loss: 0.6856 | Val Loss: 2.7604
Epoch 18 | Train Loss: 0.6855 | Val Loss: 2.7534
Epoch 19 | Train Loss: 0.6859 | Val Loss: 2.7101
Epoch 20 | Train Loss: 0.6865 | Val Loss: 2.7290
Epoch 21 | Train Loss: 0.6861 | Val Loss: 2.6869
Epoch 22 | Train Loss: 0.6859 | Val Loss: 2.7933
Epoch 23 | Train Loss: 0.6862 | Val Loss: 2.7111
Epoch 24 | Train Loss: 0.6857 | Val Loss: 2.7496
Epoch 25 | Train Loss: 0.6863 | Val Loss: 2.7316
Epoch 26 | Train Loss: 0.6856 | Val Loss: 2.7652
Epoch 27 | Train Loss: 0.6856 | Val Loss: 2.7206
Epoch 28 | Train Loss: 0.6868 | Val Loss: 2.7443
Epoch 29 | Train Loss: 0.6858 | Val Loss: 2.7550
Epoch 30 | Train Loss: 0.6859 | Val Loss: 2.7386
Epoch 31 | Train Loss: 0.6861 | Val Loss: 2.7406
Epoch 32 | Train Loss: 0.6860 | Val Loss: 2.7149
Epoch 33 | Train Loss: 0.6854 | Val Loss: 2.7959
Epoch 34 | Train Loss: 0.6862 | Val Loss: 2.6983
Epoch 35 | Train Loss: 0.6853 | Val Loss: 2.7309
Epoch 36 | Train Loss: 0.6859 | Val Loss: 2.7272
Epoch 37 | Train Loss: 0.6869 | Val Loss: 2.7385
Epoch 38 | Train Loss: 0.6854 | Val Loss: 2.7558
Epoch 39 | Train Loss: 0.6854 | Val Loss: 2.7694
Epoch 40 | Train Loss: 0.6853 | Val Loss: 2.7329
Epoch 41 | Train Loss: 0.6863 | Val Loss: 2.7056
Epoch 42 | Train Loss: 0.6856 | Val Loss: 2.7148
Epoch 43 | Train Loss: 0.6859 | Val Loss: 2.7564
Epoch 44 | Train Loss: 0.6866 | Val Loss: 2.7366
Epoch 45 | Train Loss: 0.6855 | Val Loss: 2.7479
Epoch 46 | Train Loss: 0.6862 | Val Loss: 2.7311
Epoch 47 | Train Loss: 0.6858 | Val Loss: 2.7110
Epoch 48 | Train Loss: 0.6855 | Val Loss: 2.7391
Epoch 49 | Train Loss: 0.6858 | Val Loss: 2.6886
Epoch 50 | Train Loss: 0.6851 | Val Loss: 2.7613
Epoch 51 | Train Loss: 0.6856 | Val Loss: 2.7148
Epoch 52 | Train Loss: 0.6858 | Val Loss: 2.7483
Epoch 53 | Train Loss: 0.6871 | Val Loss: 2.6904
Epoch 54 | Train Loss: 0.6861 | Val Loss: 2.7031
Epoch 55 | Train Loss: 0.6870 | Val Loss: 2.7394
Epoch 56 | Train Loss: 0.6857 | Val Loss: 2.7492
Epoch 57 | Train Loss: 0.6859 | Val Loss: 2.7011
Epoch 58 | Train Loss: 0.6854 | Val Loss: 2.7676
Epoch 59 | Train Loss: 0.6862 | Val Loss: 2.7160
Epoch 60 | Train Loss: 0.6861 | Val Loss: 2.6878
Epoch 61 | Train Loss: 0.6859 | Val Loss: 2.7341
Epoch 62 | Train Loss: 0.6860 | Val Loss: 2.7355
Epoch 63 | Train Loss: 0.6858 | Val Loss: 2.7169
Epoch 64 | Train Loss: 0.6864 | Val Loss: 2.7364
Epoch 65 | Train Loss: 0.6857 | Val Loss: 2.6881
Epoch 66 | Train Loss: 0.6863 | Val Loss: 2.7047
Epoch 67 | Train Loss: 0.6859 | Val Loss: 2.6988
Epoch 68 | Train Loss: 0.6857 | Val Loss: 2.7472
Epoch 69 | Train Loss: 0.6856 | Val Loss: 2.7357
Epoch 70 | Train Loss: 0.6855 | Val Loss: 2.7280
Epoch 71 | Train Loss: 0.6865 | Val Loss: 2.8027
Epoch 72 | Train Loss: 0.6861 | Val Loss: 2.7436
Epoch 73 | Train Loss: 0.6858 | Val Loss: 2.7145
Epoch 74 | Train Loss: 0.6857 | Val Loss: 2.7359
Epoch 75 | Train Loss: 0.6858 | Val Loss: 2.7453
Epoch 76 | Train Loss: 0.6855 | Val Loss: 2.7508
Epoch 77 | Train Loss: 0.6861 | Val Loss: 2.6952
Epoch 78 | Train Loss: 0.6857 | Val Loss: 2.7107
Epoch 79 | Train Loss: 0.6857 | Val Loss: 2.6998
Epoch 80 | Train Loss: 0.6856 | Val Loss: 2.7322
Epoch 81 | Train Loss: 0.6855 | Val Loss: 2.7455
Epoch 82 | Train Loss: 0.6863 | Val Loss: 2.7871
Epoch 83 | Train Loss: 0.6853 | Val Loss: 2.6998
Epoch 84 | Train Loss: 0.6860 | Val Loss: 2.8040
Epoch 85 | Train Loss: 0.6857 | Val Loss: 2.6813
Epoch 86 | Train Loss: 0.6859 | Val Loss: 2.7640
Epoch 87 | Train Loss: 0.6863 | Val Loss: 2.7046
Epoch 88 | Train Loss: 0.6854 | Val Loss: 2.7727
Epoch 89 | Train Loss: 0.6854 | Val Loss: 2.7285
Epoch 90 | Train Loss: 0.6857 | Val Loss: 2.7310
Epoch 91 | Train Loss: 0.6856 | Val Loss: 2.7347
Epoch 92 | Train Loss: 0.6863 | Val Loss: 2.6835
Epoch 93 | Train Loss: 0.6858 | Val Loss: 2.6901
Epoch 94 | Train Loss: 0.6859 | Val Loss: 2.7541
Epoch 95 | Train Loss: 0.6856 | Val Loss: 2.7442
Epoch 96 | Train Loss: 0.6855 | Val Loss: 2.7674
Epoch 97 | Train Loss: 0.6854 | Val Loss: 2.7089
Epoch 98 | Train Loss: 0.6854 | Val Loss: 2.6958
Epoch 99 | Train Loss: 0.6866 | Val Loss: 2.6724
Epoch 100 | Train Loss: 0.6858 | Val Loss: 2.7095
Epoch 101 | Train Loss: 0.6853 | Val Loss: 2.7618
Epoch 102 | Train Loss: 0.6858 | Val Loss: 2.7242
Epoch 103 | Train Loss: 0.6855 | Val Loss: 2.7345
Epoch 104 | Train Loss: 0.6863 | Val Loss: 2.7002
Epoch 105 | Train Loss: 0.6863 | Val Loss: 2.7739
Epoch 106 | Train Loss: 0.6869 | Val Loss: 2.7014
Epoch 107 | Train Loss: 0.6864 | Val Loss: 2.6971
Epoch 108 | Train Loss: 0.6853 | Val Loss: 2.7858
Epoch 109 | Train Loss: 0.6858 | Val Loss: 2.7671
Epoch 110 | Train Loss: 0.6855 | Val Loss: 2.7318
Epoch 111 | Train Loss: 0.6856 | Val Loss: 2.7091
Epoch 112 | Train Loss: 0.6858 | Val Loss: 2.7079
Epoch 113 | Train Loss: 0.6857 | Val Loss: 2.7394
Epoch 114 | Train Loss: 0.6852 | Val Loss: 2.7261
Epoch 115 | Train Loss: 0.6857 | Val Loss: 2.7517
Epoch 116 | Train Loss: 0.6857 | Val Loss: 2.7216
Epoch 117 | Train Loss: 0.6857 | Val Loss: 2.7225
Epoch 118 | Train Loss: 0.6861 | Val Loss: 2.7285
Epoch 119 | Train Loss: 0.6859 | Val Loss: 2.7251
Epoch 120 | Train Loss: 0.6858 | Val Loss: 2.6741
Epoch 121 | Train Loss: 0.6861 | Val Loss: 2.7134
Epoch 122 | Train Loss: 0.6856 | Val Loss: 2.7199
Epoch 123 | Train Loss: 0.6863 | Val Loss: 2.7110
Epoch 124 | Train Loss: 0.6864 | Val Loss: 2.7601
Epoch 125 | Train Loss: 0.6858 | Val Loss: 2.7487
Epoch 126 | Train Loss: 0.6858 | Val Loss: 2.7346
Epoch 127 | Train Loss: 0.6860 | Val Loss: 2.6897
Epoch 128 | Train Loss: 0.6855 | Val Loss: 2.7021
Epoch 129 | Train Loss: 0.6858 | Val Loss: 2.7371
Epoch 130 | Train Loss: 0.6858 | Val Loss: 2.7172
Epoch 131 | Train Loss: 0.6867 | Val Loss: 2.7441
Epoch 132 | Train Loss: 0.6861 | Val Loss: 2.7304
Epoch 133 | Train Loss: 0.6856 | Val Loss: 2.7112
Epoch 134 | Train Loss: 0.6855 | Val Loss: 2.7570
Epoch 135 | Train Loss: 0.6862 | Val Loss: 2.7204
Epoch 136 | Train Loss: 0.6858 | Val Loss: 2.6860
Epoch 137 | Train Loss: 0.6864 | Val Loss: 2.7311
Epoch 138 | Train Loss: 0.6857 | Val Loss: 2.7392
Epoch 139 | Train Loss: 0.6862 | Val Loss: 2.7183
Epoch 140 | Train Loss: 0.6860 | Val Loss: 2.6990
Epoch 141 | Train Loss: 0.6866 | Val Loss: 2.7509
Epoch 142 | Train Loss: 0.6859 | Val Loss: 2.7612
Epoch 143 | Train Loss: 0.6857 | Val Loss: 2.7687
Epoch 144 | Train Loss: 0.6866 | Val Loss: 2.7188
Epoch 145 | Train Loss: 0.6856 | Val Loss: 2.7275
Epoch 146 | Train Loss: 0.6858 | Val Loss: 2.8365
Epoch 147 | Train Loss: 0.6868 | Val Loss: 2.7415
Epoch 148 | Train Loss: 0.6856 | Val Loss: 2.7015
Epoch 149 | Train Loss: 0.6864 | Val Loss: 2.7637
Epoch 150 | Train Loss: 0.6862 | Val Loss: 2.7687
Epoch 151 | Train Loss: 0.6859 | Val Loss: 2.7093
Epoch 152 | Train Loss: 0.6858 | Val Loss: 2.7838
Epoch 153 | Train Loss: 0.6858 | Val Loss: 2.7862
Epoch 154 | Train Loss: 0.6857 | Val Loss: 2.6983
Epoch 155 | Train Loss: 0.6858 | Val Loss: 2.7616
Epoch 156 | Train Loss: 0.6858 | Val Loss: 2.7473
Epoch 157 | Train Loss: 0.6859 | Val Loss: 2.7343
Epoch 158 | Train Loss: 0.6855 | Val Loss: 2.7597
Epoch 159 | Train Loss: 0.6863 | Val Loss: 2.7916
Epoch 160 | Train Loss: 0.6858 | Val Loss: 2.7043
Epoch 161 | Train Loss: 0.6861 | Val Loss: 2.7448
Epoch 162 | Train Loss: 0.6858 | Val Loss: 2.7398
Epoch 163 | Train Loss: 0.6860 | Val Loss: 2.7550
Epoch 164 | Train Loss: 0.6862 | Val Loss: 2.6838
Epoch 165 | Train Loss: 0.6864 | Val Loss: 2.7562
Epoch 166 | Train Loss: 0.6855 | Val Loss: 2.6791
Epoch 167 | Train Loss: 0.6861 | Val Loss: 2.7385
Epoch 168 | Train Loss: 0.6857 | Val Loss: 2.8040
Epoch 169 | Train Loss: 0.6863 | Val Loss: 2.7006
Epoch 170 | Train Loss: 0.6868 | Val Loss: 2.7175
Epoch 171 | Train Loss: 0.6852 | Val Loss: 2.7053
Epoch 172 | Train Loss: 0.6858 | Val Loss: 2.7219
Epoch 173 | Train Loss: 0.6856 | Val Loss: 2.7009
Epoch 174 | Train Loss: 0.6861 | Val Loss: 2.7584
Epoch 175 | Train Loss: 0.6862 | Val Loss: 2.7134
Epoch 176 | Train Loss: 0.6854 | Val Loss: 2.7087
Epoch 177 | Train Loss: 0.6864 | Val Loss: 2.8005
Epoch 178 | Train Loss: 0.6858 | Val Loss: 2.7163
Epoch 179 | Train Loss: 0.6866 | Val Loss: 2.7138
Epoch 180 | Train Loss: 0.6856 | Val Loss: 2.7101
Epoch 181 | Train Loss: 0.6857 | Val Loss: 2.7716
Epoch 182 | Train Loss: 0.6860 | Val Loss: 2.7135
Epoch 183 | Train Loss: 0.6855 | Val Loss: 2.7542
Epoch 184 | Train Loss: 0.6853 | Val Loss: 2.7264
Epoch 185 | Train Loss: 0.6858 | Val Loss: 2.7252
Epoch 186 | Train Loss: 0.6857 | Val Loss: 2.7430
Epoch 187 | Train Loss: 0.6862 | Val Loss: 2.7410
Epoch 188 | Train Loss: 0.6856 | Val Loss: 2.7137
Epoch 189 | Train Loss: 0.6860 | Val Loss: 2.7988
Epoch 190 | Train Loss: 0.6854 | Val Loss: 2.7736
Epoch 191 | Train Loss: 0.6862 | Val Loss: 2.7238
Epoch 192 | Train Loss: 0.6857 | Val Loss: 2.7907
Epoch 193 | Train Loss: 0.6860 | Val Loss: 2.7926
Epoch 194 | Train Loss: 0.6862 | Val Loss: 2.7282
Epoch 195 | Train Loss: 0.6856 | Val Loss: 2.7692
Epoch 196 | Train Loss: 0.6866 | Val Loss: 2.7054
Epoch 197 | Train Loss: 0.6858 | Val Loss: 2.7168
Epoch 198 | Train Loss: 0.6861 | Val Loss: 2.6954
Epoch 199 | Train Loss: 0.6859 | Val Loss: 2.7455
Early stopping.
Test Accuracy: 0.5499
Test PRAUC: 0.5523
Results saved to /cluster/projects/mcintoshgroup/publicData/fine-grain/experiment/mgca_res50_results.csv
Some weights of the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ were not used when initializing BertModel: ['encoder.layer.11.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /cluster/projects/mcintoshgroup/publicData/fine-grain/CXR-CLIP-Text-Encoder/ and are newly initialized: ['encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
